apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: collector-for-gpu
  name: collector-for-gpu
  namespace: nvidia-gpu-operator
  annotations:
    openshift.io/scc: collector-for-gpu
spec:
  selector:
    matchLabels:
      app: collector-for-gpu
  template:
    metadata:
      labels:
        app: collector-for-gpu
    spec:
      nodeSelector:
        nvidia.com/gpu.deploy.dcgm-exporter: "true"
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
#      priorityClassName: system-node-critical
      serviceAccountName: nvidia-dcgm-exporter
#      initContainers:
#      - name: toolkit-validation
#        image: "FILLED BY THE OPERATOR"
#        command: ['sh', '-c']
#        args: ["until [ -f /run/nvidia/validations/toolkit-ready ]; do echo waiting for nvidia container stack to be setup; sleep 5; done"]
#        securityContext:
#          privileged: true
#        volumeMounts:
#          - name: run-nvidia
#            mountPath: "/run/nvidia"
#            mountPropagation: HostToContainer
      containers:
      - image: "ghcr.io/oavner/collector-for-gpu:2024-01-09-14-49-52"
        name: collector-for-gpu
        args: ["nvidia-gpu-operator"]
#      containers:
#      - image: "nvidia/cuda:12.3.1-runtime-ubuntu20.04"
#        name: collector-for-gpu
#        command: ['sh', '-c']
#        args: ["sleep 3600000000"]
#        env:
#        - name: DCGM_EXPORTER_LISTEN
#          value: ":9400"
#        - name: DCGM_EXPORTER_KUBERNETES
#          value: "true"
#        - name: DCGM_EXPORTER_COLLECTORS
#          value: "/etc/dcgm-exporter/dcp-metrics-included.csv"
#        - name: NODE_NAME
#          valueFrom:
#            fieldRef:
#              fieldPath: spec.nodeName
#        ports:
#        - name: "metrics"
#          containerPort: 9400
#        volumeMounts:
#        - name: "pod-gpu-resources"
#          readOnly: true
#          mountPath: "/var/lib/kubelet/pod-resources"
        securityContext:
          privileged: true
#      volumes:
#      - name: "pod-gpu-resources"
#        hostPath:
#          path: "/var/lib/kubelet/pod-resources"
#      - name: run-nvidia
#        hostPath:
#          path: /run/nvidia